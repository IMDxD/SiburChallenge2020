{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- beautifulsoup4==4.7.1\n",
    "- catboost==0.24.2\n",
    "- geonamescache==1.2.0\n",
    "- lxml==4.6.2\n",
    "- numpy==1.18.5\n",
    "- pandas==1.1.3\n",
    "- pycountry==20.7.3\n",
    "- scikit-learn==0.21.3\n",
    "- spacy==2.3.4\n",
    "- unidecode==1.1.1\n",
    "\n",
    "RUN python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import *\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import spacy\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import unidecode\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp_en = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"\")\n",
    "train = pd.read_csv(DATA_DIR.joinpath('train.csv'), index_col=\"pair_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs = dict()\n",
    "for i, s in enumerate(set(train['name_1']) | set(train['name_2'])):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    docs[s] = nlp_en(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translitMapRuEn = {\n",
    "    \"инк\": [\"inc\"],\n",
    "    \"ай\": [\"i\"],\n",
    "    \"аш\": [\"ussi\", \"h\"],\n",
    "    \"дж\": [\"g\", \"j\"],\n",
    "    \"кс\": [\"x\", \"ks\", \"cs\"],\n",
    "    \"а\": [\"a\"],\n",
    "    \"б\": [\"b\"],\n",
    "    \"в\": [\"v\", \"w\"],\n",
    "    \"г\": [\"g\"],\n",
    "    \"д\": [\"d\"],\n",
    "    \"е\": [\"e\"],\n",
    "    \"ё\": [\"e\", \"eu\"],\n",
    "    \"ж\": [\"zh\"],\n",
    "    \"з\": [\"z\"],\n",
    "    \"и\": [\"i\", \"e\"],\n",
    "    \"й\": [\"i\", \"y\"],\n",
    "    \"к\": [\"k\", \"c\", \"q\"],\n",
    "    \"л\": [\"l\"],\n",
    "    \"м\": [\"m\"],\n",
    "    \"н\": [\"n\"],\n",
    "    \"о\": [\"o\"],\n",
    "    \"п\": [\"p\"],\n",
    "    \"р\": [\"r\"],\n",
    "    \"с\": [\"s\", \"c\"],\n",
    "    \"т\": [\"t\", \"th\"],\n",
    "    \"у\": [\"u\", \"oo\"],\n",
    "    \"ф\": [\"f\", \"ph\"],\n",
    "    \"х\": [\"kh\", \"h\"],\n",
    "    \"ц\": [\"ts\", \"c\"],\n",
    "    \"ч\": [\"ch\", \"c\"],\n",
    "    \"ш\": [\"sh\"],\n",
    "    \"щ\": [\"shch\"],\n",
    "    \"ъ\": [\"ie\", \"\"],\n",
    "    \"ы\": [\"y\"],\n",
    "    \"ь\": [\"\"],\n",
    "    \"э\": [\"e\", \"a\"],\n",
    "    \"ю\": [\"iu\", \"u\"],\n",
    "    \"я\": [\"ia\", \"ya\"],\n",
    "}\n",
    "\n",
    "def translit(\n",
    "    word: str,\n",
    "    translit_map,\n",
    "    results_limit = 1,\n",
    "    add_silent_e: bool = False,\n",
    "):\n",
    "    if len(word) == 0 and add_silent_e:\n",
    "        return {\"\", \"e\"}\n",
    "    elif len(word) == 0:\n",
    "        return {\"\"}\n",
    "\n",
    "    res = set()\n",
    "\n",
    "    # If character is not in a mapping table leave it as is\n",
    "    if word[0] not in translit_map:\n",
    "        tres = translit(word[1:], translit_map, results_limit, add_silent_e)\n",
    "        for replaced_tail in tres:\n",
    "            res.add(word[0] + replaced_tail)\n",
    "        return res\n",
    "\n",
    "    # Recursively add all possible transliteration combinations\n",
    "    for k, v in sorted(translit_map.items(), key=lambda x: -len(x[0])):\n",
    "        if word.startswith(k):\n",
    "            tres = translit(word[len(k) :], translit_map, results_limit, add_silent_e)\n",
    "            for replaced_tail in tres:\n",
    "                for replacement_variant in v:\n",
    "                    if results_limit is not None and len(res) >= results_limit:\n",
    "                        return res\n",
    "                    res.add(replacement_variant + replaced_tail)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_alphanum_regex = re.compile(\"[^0-9a-zA-Zа-яА-ЯёЁ ]+\")\n",
    "\n",
    "def simple_transform(s, del_brackets=True):\n",
    "    s0 = s\n",
    "    s = s.lower()\n",
    "    s = s.replace('ооо', '').replace('ооо', '').replace('oao', '').replace('оао', '')\n",
    "    s = s.replace('ОБЩЕСТВО С ОГРАНИЧЕННОЙ ОТВЕТСТВЕННОСТЬЮ'.lower(), '')\n",
    "    s = s.replace('ь', '')\n",
    "    s = list(translit(s, translitMapRuEn))[0]\n",
    "    s = unidecode.unidecode(s)\n",
    "\n",
    "    s = s.replace(',', ' ').replace('.', '').replace('*', '') \\\n",
    "        .replace('\"', ' ').replace(\"'\", ' ').replace('-', ' ').replace('&', ' ') \\\n",
    "        .replace('\\\\', '').replace('?', ' ')\n",
    "    s = s.replace('(', ' (')\n",
    "    s = s.replace(')', ') ')\n",
    "    s = s.replace('( ', '(')\n",
    "    s = s.replace(' )', ')')\n",
    "    if len(s.strip()) > 0 and s.strip()[0] == '(':\n",
    "        s = s.strip()[1:]\n",
    "    if del_brackets:\n",
    "        s = re.sub(\"\\(.*\\)\", \"\", s)\n",
    "    s = s.replace('(', '').replace(')', '')\n",
    "    s = re.sub(non_alphanum_regex, '', s)\n",
    "    #s = legal_re.sub('', s)\n",
    "    s = ' '.join(s.split())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = requests.get('https://en.wikipedia.org/wiki/List_of_legal_entity_types_by_country')\n",
    "# soup = BeautifulSoup(res.content, \"lxml\")\n",
    "\n",
    "# with open('legal_entity_types_by_country.txt', 'w', encoding='utf-8') as fout:\n",
    "#     fout.write(str(soup))\n",
    "\n",
    "\n",
    "with open('legal_entity_types_by_country.txt', 'r', encoding='utf-8') as fin:\n",
    "    s = fin.read()\n",
    "soup = BeautifulSoup(s, \"lxml\")\n",
    "\n",
    "countries0 = []\n",
    "\n",
    "legal = set()\n",
    "for e in soup.find_all('h2')[1:]:\n",
    "    country = e.find('span').text\n",
    "    countries0.append(country.lower())\n",
    "    if country == 'See also':\n",
    "        break\n",
    "#     print()\n",
    "    for ee in e.findNext('ul').find_all('li'):\n",
    "        v = ee.text.split('(')[0].split(':')[0].split('≈')[0].split('=')[0].split('–')[0]\n",
    "        for vv in v.split('/'):\n",
    "            vv = vv.strip()\n",
    "            legal.add(vv.lower())\n",
    "            legal.add(vv.lower().replace('.', ''))\n",
    "#    print()\n",
    "\n",
    "legal.add('pvt')\n",
    "legal |= {'de', 'international', 'industries', 'industria', 'imp', 'exp'}\n",
    "\n",
    "len(legal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "300\n",
      "350\n",
      "23134\n",
      "263065\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "import geonamescache\n",
    "\n",
    "gc = geonamescache.GeonamesCache()\n",
    "\n",
    "countries = countries0 + [country.name.lower() for country in pycountry.countries]\n",
    "countries.append('usa')\n",
    "countries.append('africa')\n",
    "countries.append('asia')\n",
    "countries.append('europe')\n",
    "countries.append('america')\n",
    "countries.append('north')\n",
    "countries.append('south')\n",
    "countries.append('west')\n",
    "countries.append('east')\n",
    "countries.append('city')\n",
    "countries.append('area')\n",
    "\n",
    "countries = set(countries)\n",
    "print(len(countries))\n",
    "\n",
    "for k, v in gc.get_countries().items():\n",
    "    c = simple_transform(v['name'])\n",
    "    if c not in countries:\n",
    "        countries.add(c)\n",
    "print(len(countries))\n",
    "        \n",
    "for k, v in gc.get_us_states().items():\n",
    "    c = simple_transform(v['name'])\n",
    "    if c not in countries:\n",
    "        countries.add(c)\n",
    "print(len(countries))\n",
    "        \n",
    "cities = set()\n",
    "for k, v in gc.get_cities().items():\n",
    "    c = simple_transform(v['name'])\n",
    "    cities.add(c)\n",
    "print(len(cities))\n",
    "\n",
    "cities_alt = set()\n",
    "for k, v in gc.get_cities().items():\n",
    "    c = simple_transform(v['name'])\n",
    "    cities_alt.add(c)\n",
    "    for e in v['alternatenames']:\n",
    "        c = simple_transform(e)\n",
    "        cities_alt.add(e)\n",
    "print(len(cities_alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_tokens = set()\n",
    "for e in legal:\n",
    "    legal_tokens.add(re.sub(non_alphanum_regex, '', e))\n",
    "    for t in e.split():\n",
    "        ut = unidecode.unidecode(t)\n",
    "        for tt in [t, re.sub(non_alphanum_regex, '', t),\n",
    "                   ut, re.sub(non_alphanum_regex, '', ut)]:\n",
    "            if len(tt) > 2 and tt not in legal and tt not in legal_tokens:\n",
    "                #print(tt)\n",
    "                legal_tokens.add(tt)\n",
    "len(legal_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sibur     pvgmbh  b.v. bova '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_str_replace(strings, debug=True):\n",
    "    re_str = r'\\b(?:' + '|'.join(\n",
    "        [re.escape(s) for s in strings]\n",
    "    ) + r')(?!\\S)'\n",
    "    if debug:\n",
    "        print(re_str)\n",
    "    return re.compile(re_str, re.UNICODE)\n",
    "\n",
    "legal_re = multi_str_replace([rf\"{entity}\" for entity in legal | legal_tokens if len(entity) > 1], debug=False)\n",
    "countries_re = multi_str_replace([rf\"{entity}\" for entity in countries], debug=False)\n",
    "\n",
    "t = r'sibur    gmbh inc. gmbh inc pvgmbh  b.v. bova inc.'\n",
    "t = legal_re.sub('', t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16380"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = Counter()\n",
    "for e in docs:\n",
    "    all_tokens.update(simple_transform(e).split())\n",
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16380\n",
      "1000 16380\n",
      "2000 16380\n",
      "3000 16380\n",
      "4000 16380\n",
      "5000 16380\n",
      "6000 16380\n",
      "7000 16380\n",
      "8000 16380\n",
      "9000 16380\n",
      "10000 16380\n",
      "11000 16380\n",
      "12000 16380\n",
      "13000 16380\n",
      "14000 16380\n",
      "15000 16380\n",
      "16000 16380\n"
     ]
    }
   ],
   "source": [
    "docs_tokens = dict()\n",
    "for i, token in enumerate(all_tokens):\n",
    "    docs_tokens[token] = nlp_en(token)\n",
    "    if i % 1000 == 0:\n",
    "        print(i, len(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869 501\n"
     ]
    }
   ],
   "source": [
    "geo_add = set()\n",
    "kk = 0\n",
    "for token in docs_tokens:\n",
    "    doc = docs_tokens[token]\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'GPE':\n",
    "            geo_add.add(token)\n",
    "            if token not in cities_alt and token not in countries:\n",
    "                #print(token)\n",
    "                kk += 1\n",
    "\n",
    "print(len(geo_add), kk)\n",
    "geo_re = multi_str_replace([rf\"{entity}\" for entity in geo_add], debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_s2tokens = dict()\n",
    "global_s2tokens[(0, 0, 0)] = dict()\n",
    "global_s2tokens[(1, 0, 0)] = dict()\n",
    "#global_s2tokens[(1, 1, 0)] = dict()\n",
    "global_s2tokens[(1, 0, 1)] = dict()\n",
    "\n",
    "def tokenize(s, del_brackets=True, only_org=False, use_freq=False):\n",
    "    h = (int(del_brackets), int(only_org), int(use_freq))\n",
    "    if s not in global_s2tokens[h]:\n",
    "#         for ent in docs[s].ents:\n",
    "#             if ent.label_ == 'GPE':\n",
    "#                 s_new = s.replace(ent.text, '')\n",
    "#                 if len(s_new) != 0:\n",
    "#                     s = s_new\n",
    "        s1 = simple_transform(s, del_brackets=del_brackets)\n",
    "        s2 = legal_re.sub('', s1)\n",
    "        s3 = countries_re.sub('', s2)\n",
    "        s4 = geo_re.sub('', s3)\n",
    "        \n",
    "        arr = s4.split()\n",
    "\n",
    "        arr_new = []\n",
    "        i = 0\n",
    "        while i < len(arr) - 1:\n",
    "            ss = arr[i] + ' ' + arr[i + 1]\n",
    "            ss1 = arr[i] + arr[i + 1]\n",
    "            if ss in cities_alt:# or ss1 in legal_tokens and len(ss1) > 2:\n",
    "                #print(arr[i] + ' ' + arr[i + 1])\n",
    "#                 if ss in cities_alt:\n",
    "#                     new_cities.update([ss])\n",
    "                i += 1\n",
    "            else:\n",
    "                arr_new.append(arr[i])\n",
    "            i += 1\n",
    "\n",
    "        if i == len(arr) - 1:\n",
    "            arr_new.append(arr[-1])\n",
    "        arr = list(arr_new)\n",
    "        \n",
    "        if only_org:\n",
    "            arr = [e for e in arr if e in org_tokens]\n",
    "        \n",
    "\n",
    "        s5 = ' '.join(arr)\n",
    "\n",
    "        s6 = ''\n",
    "        last = None\n",
    "        for ch in s5:\n",
    "            if last != ch:\n",
    "                last = ch\n",
    "                s6 += ch\n",
    "        \n",
    "        if use_freq and len(arr) > 0:\n",
    "            arr = s6.split()\n",
    "            arr = sorted(arr, key=lambda x: -tokens_freq.get(x, 0))[:1]\n",
    "            s6 = ' '.join(arr)\n",
    "        \n",
    "        global_s2tokens[h][s] = None\n",
    "        if only_org:\n",
    "            global_s2tokens[h][s] = s6.split()\n",
    "        else:\n",
    "            for ss in [s6, s5, s4, s3, s2, s1, s]:\n",
    "                res = ss.split()\n",
    "                if len(res) != 0:\n",
    "                    global_s2tokens[h][s] = res\n",
    "                    break\n",
    "        if global_s2tokens[h][s] is None:\n",
    "            print(s)\n",
    "            raise\n",
    "    return global_s2tokens[h][s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(train, use_simple=False):\n",
    "    clusters = list()\n",
    "    k2ind = dict()\n",
    "\n",
    "    freq = defaultdict(lambda: [0, 0, 0, 0, 0, 0, 0])\n",
    "    cl2cl_neg = defaultdict(int)\n",
    "    for t1, t1s, t2, t2s, y in zip(train['name_1_tokens'], train['name_1_tokens_simple'],\n",
    "                                   train['name_2_tokens'], train['name_2_tokens_simple'],\n",
    "                                   train['is_duplicate']):\n",
    "        if not use_simple:\n",
    "            st1 = set(t1)\n",
    "            st2 = set(t2)\n",
    "        else:\n",
    "            st1 = t1s\n",
    "            st2 = t2s\n",
    "\n",
    "        k1 = tuple(sorted(st1))\n",
    "        k2 = tuple(sorted(st2))\n",
    "        c1 = k2ind.get(k1)\n",
    "        c2 = k2ind.get(k2)\n",
    "        if y == 1:\n",
    "#             c1 = c2 = None\n",
    "#             for i, c in enumerate(clusters):\n",
    "#                 if k1 in c:\n",
    "#                     c1 = i\n",
    "#                 if k2 in c:\n",
    "#                     c2 = i\n",
    "            if c1 is not None:\n",
    "                if c2 is not None:\n",
    "                    if c1 != c2:\n",
    "                        clusters[c1] |= clusters[c2]\n",
    "                        for e in clusters[c2]:\n",
    "                            k2ind[e] = c1\n",
    "                        clusters[c2] = set()\n",
    "                else:\n",
    "                    clusters[c1] |= set([k2])\n",
    "                    k2ind[k2] = c1\n",
    "            else:\n",
    "#                if c1 is not None:\n",
    "                if c2 is not None:\n",
    "                    clusters[c2] |= set([k1])\n",
    "                    k2ind[k1] = c2\n",
    "                else:\n",
    "                    clusters.append({k1, k2})\n",
    "                    k2ind[k1] = len(clusters) - 1\n",
    "                    k2ind[k2] = len(clusters) - 1\n",
    "#         else:\n",
    "#             if c1 is None:\n",
    "#                 clusters.append({k1})\n",
    "#                 k2ind[k1] = len(clusters) - 1\n",
    "#             if c2 is None:\n",
    "#                 clusters.append({k2})\n",
    "#                 k2ind[k2] = len(clusters) - 1\n",
    "                \n",
    "\n",
    "        for w1 in t1:\n",
    "            freq[w1][0] += 1        \n",
    "            if w1 not in t2:\n",
    "                freq[w1][1 + y] += 1\n",
    "            else:\n",
    "                freq[w1][3 + y] += 1\n",
    "        for w2 in t2:\n",
    "            freq[w2][0] += 1        \n",
    "            if w2 not in t1:\n",
    "                freq[w2][1 + y] += 1\n",
    "            else:\n",
    "                freq[w2][3 + y] += 1\n",
    "\n",
    "    for t1, t1s, t2, t2s, y in zip(train['name_1_tokens'], train['name_1_tokens_simple'],\n",
    "                                   train['name_2_tokens'], train['name_2_tokens_simple'],\n",
    "                                   train['is_duplicate']):\n",
    "        if not use_simple:\n",
    "            st1 = set(t1)\n",
    "            st2 = set(t2)\n",
    "        else:\n",
    "            st1 = t1s\n",
    "            st2 = t2s\n",
    "        k1 = tuple(sorted(st1))\n",
    "        k2 = tuple(sorted(st2))\n",
    "        c1 = k2ind.get(k1)\n",
    "        c2 = k2ind.get(k2)\n",
    "        if y == 0:\n",
    "            cl2cl_neg[(c1, c2)] += 1\n",
    "            cl2cl_neg[(c2, c1)] += 1\n",
    "                \n",
    "    for c in clusters:\n",
    "        cc = list(c)\n",
    "        for i1 in range(len(c)):\n",
    "            for i2 in range(i1 + 1, len(c)):\n",
    "                for t1, t2 in [(cc[i1], cc[i2]), (cc[i2], cc[i1])]:\n",
    "                    for w1 in t1:\n",
    "                        if w1 not in t2:\n",
    "                            freq[w1][5] += 1\n",
    "                        else:\n",
    "                            freq[w1][6] += 1\n",
    "    \n",
    "    return clusters, k2ind, freq, cl2cl_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_freq = Counter()\n",
    "for e in set(train['name_1']):\n",
    "    tokens_freq.update(tokenize(e)[:2])\n",
    "for e in set(train['name_2']):\n",
    "    tokens_freq.update(tokenize(e)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name_1\n",
      "name_2\n",
      "Wall time: 28.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for col in ['name_1', 'name_2']:\n",
    "    print(col)\n",
    "    train[f'{col}_tokens'] = [tokenize(e) for e in train[col]]\n",
    "    train[f'{col}_tokens_with_br'] = [tokenize(e, del_brackets=False) for e in train[col]]\n",
    "    train[f'{col}_tokens_simple'] = [simple_transform(e).split() for e in train[col]] \n",
    "#     train[f'{col}_tokens_org'] = [tokenize(e, del_brackets=True, only_org=True) for e in train[col]]\n",
    "    train[f'{col}_tokens_freq'] = [tokenize(e, del_brackets=True, only_org=False, use_freq=True) for e in train[col]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clusters, k2ind, freq, cl2cl_neg = prepare(train)\n",
    "clusters_simple, k2ind_simple, freq_simple, _ = prepare(train, use_simple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cv'] = -np.arange(1, len(train) + 1)\n",
    "ind_positive = (\n",
    "    train['name_1_tokens_simple'].apply(lambda x: k2ind_simple.get(tuple(sorted(set(x))))) == \n",
    "    train['name_2_tokens_simple'].apply(lambda x: k2ind_simple.get(tuple(sorted(set(x)))))\n",
    ")\n",
    "train.loc[ind_positive, 'cv'] = train.loc[ind_positive, 'name_1_tokens_simple'] \\\n",
    "    .apply(lambda x: k2ind_simple.get(tuple(sorted(set(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 81        645\n",
       " 106       310\n",
       " 68        214\n",
       " 33        177\n",
       " 66        140\n",
       "          ... \n",
       "-117396      1\n",
       "-127637      1\n",
       "-129686      1\n",
       "-123543      1\n",
       "-225426      1\n",
       "Name: cv, Length: 494775, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cv'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_is_one_cluster(t1, t2, clusters, k2ind):\n",
    "    st1 = set(t1)\n",
    "    st2 = set(t2)\n",
    "    k1 = tuple(sorted(st1))\n",
    "    k2 = tuple(sorted(st2))\n",
    "    c1 = k2ind.get(k1)\n",
    "    c2 = k2ind.get(k2)\n",
    "    if c1 is not None and c2 is not None and c1 == c2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def calc_cnt_negative(t1, t2, clusters, k2ind, cl2cl_neg):\n",
    "    st1 = set(t1)\n",
    "    st2 = set(t2)\n",
    "    k1 = tuple(sorted(st1))\n",
    "    k2 = tuple(sorted(st2))\n",
    "    c1 = k2ind.get(k1)\n",
    "    c2 = k2ind.get(k2)\n",
    "    if c1 is not None and c2 is not None:\n",
    "        return cl2cl_neg.get((c1, c2), 0)\n",
    "    return -1\n",
    "\n",
    "\n",
    "def get_cluster_size(t1, clusters, k2ind):\n",
    "    st1 = set(t1)\n",
    "    k1 = tuple(sorted(st1))\n",
    "    c1 = k2ind.get(k1)\n",
    "    return len(clusters[c1]) if c1 is not None else 0\n",
    "\n",
    "\n",
    "def calc_lcs(s1, s2):\n",
    "    c = []\n",
    "    for i in range(len(s1) + 1):\n",
    "        c.append([0] * (len(s2) + 1))\n",
    "    for i in range(0, len(s1)):\n",
    "        for j in range(0, len(s2)):\n",
    "            if s1[i] == s2[j]:\n",
    "                c[i + 1][j + 1] = c[i][j] + 1\n",
    "            else:\n",
    "                c[i + 1][j + 1] = max(c[i][j + 1], c[i + 1][j])\n",
    "    return c[len(s1)][len(s2)]\n",
    "\n",
    "def expand_tokens(t):\n",
    "    res = set(t)\n",
    "    for i in range(len(t) - 1):\n",
    "        res.add(t[i] + t[i + 1])\n",
    "    for i in range(len(t) - 2):\n",
    "        res.add(t[i] + t[i + 1] + t[i + 2])\n",
    "    for i in range(len(t) - 3):\n",
    "        res.add(t[i][0] + t[i + 1][0] + t[i + 2][0])    \n",
    "    for i in range(len(t) - 4):\n",
    "        res.add(t[i][0] + t[i + 1][0] + t[i + 2][0] + t[i + 3][0])    \n",
    "    return res\n",
    "\n",
    "\n",
    "def op(a, b, agg):\n",
    "    if agg == 'min':\n",
    "        return min(a, b)\n",
    "    if agg == 'max':\n",
    "        return max(a, b)\n",
    "    if agg == 'sum':\n",
    "        return a + b\n",
    "    raise\n",
    "\n",
    "\n",
    "def calc_features(df_te,\n",
    "                  clusters_list, k2ind_list, freq_list, cl2cl_neg_list,\n",
    "                  #clusters_simple_list, k2ind_simple_list\n",
    "                 ):\n",
    "    res = df_te.copy()\n",
    "    \n",
    "    features_te = defaultdict(list)\n",
    "    \n",
    "    for i, df, df_features in [\n",
    "        (0, df_te, features_te)\n",
    "    ]:\n",
    "        print(i)\n",
    "#        for s1, t1, t1s, t1b, t1o, t1f, s2, t2, t2s, t2b, t2o, t2f in (\n",
    "        for s1, t1, t1s, t1b, t1f, s2, t2, t2s, t2b, t2f in (\n",
    "            zip(df['name_1'], df['name_1_tokens'], df['name_1_tokens_simple'],\n",
    "                df['name_1_tokens_with_br'], df['name_1_tokens_freq'],\n",
    "                df['name_2'], df['name_2_tokens'], df['name_2_tokens_simple'],\n",
    "                df['name_2_tokens_with_br'], df['name_2_tokens_freq']\n",
    "        )):\n",
    "            df_features['len_max'].append(max(len(s1), len(s2)))\n",
    "            df_features['len_min'].append(min(len(s1), len(s2)))\n",
    "            df_features['len_diff'].append(abs(len(s1) - len(s2)))\n",
    "            df_features['len_diff_rel'].append(abs(len(s1) - len(s2)) / (len(s1) + len(s2)))\n",
    "\n",
    "            df_features['len_t_max'].append(max(len(t1), len(t2)))\n",
    "            df_features['len_t_min'].append(min(len(t1), len(t2)))\n",
    "            df_features['len_t_diff'].append(abs(len(t1) - len(t2)))\n",
    "            df_features['len_t_diff_rel'].append(abs(len(t1) - len(t2)) / (len(t1) + len(t2)))\n",
    "\n",
    "            st1 = set(t1)\n",
    "            st2 = set(t2)\n",
    "            num, den = len(st1 & st2), len(st1 | st2)\n",
    "            df_features['sim_tokens_num'].append(num)\n",
    "            df_features['sim_tokens_den'].append(den)\n",
    "            df_features['sim_tokens'].append(num / den)\n",
    "            \n",
    "            st1_exp = expand_tokens(t1) | set(t1b)\n",
    "            st2_exp = expand_tokens(t2) | set(t2b)\n",
    "            num1, den1 = len(st1 & st2_exp), len(st1)\n",
    "            num2, den2 = len(st2 & st1_exp), len(st2)\n",
    "            df_features['sim_tokens_exp_min'].append(min(num1 / den1, num2 / den2))\n",
    "            df_features['sim_tokens_exp_max'].append(max(num1 / den1, num2 / den2))\n",
    "            \n",
    "            e1, e2 = ''.join(t1), ''.join(t2)\n",
    "            ll = calc_lcs(e1, e2)\n",
    "            try:\n",
    "                df_features['lcs'].append(ll)\n",
    "                df_features['lcs_norm'].append(ll / min(len(e1), len(e2)))\n",
    "                df_features['lcs_norm_max'].append(ll / max(len(e1), len(e2)))\n",
    "                df_features['lcs_norm_sum'].append(ll / (len(e1) + len(e2)))\n",
    "            except:\n",
    "                print(e1, e2, t1, t2)\n",
    "                raise\n",
    "            \n",
    "            for i in range(1, 4):\n",
    "                e1, e2 = ''.join(t1[:i]), ''.join(t2[:i])\n",
    "                ll = calc_lcs(e1, e2)\n",
    "                df_features[f'lcs{i}_norm'].append(ll / min(len(e1), len(e2)))\n",
    "\n",
    "            e1, e2 = simple_transform(s1), simple_transform(s2)\n",
    "            ll = calc_lcs(e1, e2)\n",
    "            df_features['lcs_raw_norm'].append(ll / min(len(e1), len(e2)))\n",
    "            \n",
    "#             e1, e2 = ''.join(t1o), ''.join(t2o)\n",
    "#             ll = calc_lcs(e1, e2)\n",
    "#             df_features['lcs_org_norm'].append((ll + 0.01) / (min(len(e1), len(e2)) + 0.1))\n",
    "\n",
    "            e1, e2 = ''.join(t1f), ''.join(t2f)\n",
    "            ll = calc_lcs(e1, e2)\n",
    "            df_features['lcs_freq_norm'].append((ll + 0.01) / (min(len(e1), len(e2)) + 0.1))\n",
    "            \n",
    "            c_features = defaultdict(list)\n",
    "            for clusters, k2ind, freq, cl2cl_neg in zip(\n",
    "#            for clusters, k2ind, freq, cl2cl_neg, clusters_simple, k2ind_simple in zip(\n",
    "                clusters_list, k2ind_list, freq_list, cl2cl_neg_list#, clusters_simple_list, k2ind_simple_list\n",
    "            ):\n",
    "                #c_features['is_one_cluster_simple'].append(calc_is_one_cluster(t1s, t2s, clusters_simple, k2ind_simple))\n",
    "                \n",
    "                c_features['is_one_cluster'].append(calc_is_one_cluster(t1, t2, clusters, k2ind))\n",
    "                \n",
    "                c_features['cnt_neg_cluster'].append(calc_cnt_negative(t1, t2, clusters, k2ind, cl2cl_neg))\n",
    "                c_features['cluster_size_max'].append(max(get_cluster_size(t1, clusters, k2ind), \n",
    "                                                          get_cluster_size(t2, clusters, k2ind)))\n",
    "\n",
    "                for ind in range(1, 7):\n",
    "                    for agg in ['sum', 'min', 'max']:\n",
    "                        score_u = score_i = score_d = 0\n",
    "                        if agg == 'min':\n",
    "                            score_u = score_i = score_d = 1e6\n",
    "                        for word in st1 & st2:\n",
    "                            score_i = op(score_i, (freq[word][ind] + 0.01) / (freq[word][0] + 1), agg)\n",
    "                        for word in st1 | st2:\n",
    "                            score_u = op(score_u, (freq[word][ind] + 0.01) / (freq[word][0] + 1), agg)\n",
    "                        for word in st1 ^ st2:\n",
    "                            score_d = op(score_d, (freq[word][ind] + 0.01) / (freq[word][0] + 1), agg)\n",
    "\n",
    "                        c_features[f'words_freq_sim_{agg}_{ind}_num'].append(score_i)\n",
    "                        c_features[f'words_freq_sim_{agg}_{ind}_den'].append(score_u)\n",
    "                        c_features[f'words_freq_sim_{agg}_{ind}_diff'].append(score_d)\n",
    "                        c_features[f'words_freq_sim_{agg}_{ind}_rel1'].append(score_i / score_u)\n",
    "                        c_features[f'words_freq_sim_{agg}_{ind}_rel2'].append(score_d / score_u)\n",
    "            for k, v in c_features.items():\n",
    "                if k in {'is_one_cluster', 'cluster_size_max'}:\n",
    "                    df_features[k].append(max(v))\n",
    "                else:\n",
    "                    df_features[k].append(sorted(v)[len(clusters_list) // 2])\n",
    "        \n",
    "    features = []\n",
    "    for k, v in sorted(features_te.items()):\n",
    "        features.append(k)\n",
    "        res[k] = v\n",
    "    print(features)\n",
    "    return res, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331879 165940\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_duplicate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164711</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_duplicate       0    1\n",
       "foo                      \n",
       "0             164711  593\n",
       "1                  5  631"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['cluster_size_max', 'cnt_neg_cluster', 'is_one_cluster', 'lcs', 'lcs1_norm', 'lcs2_norm', 'lcs3_norm', 'lcs_freq_norm', 'lcs_norm', 'lcs_norm_max', 'lcs_norm_sum', 'lcs_raw_norm', 'len_diff', 'len_diff_rel', 'len_max', 'len_min', 'len_t_diff', 'len_t_diff_rel', 'len_t_max', 'len_t_min', 'sim_tokens', 'sim_tokens_den', 'sim_tokens_exp_max', 'sim_tokens_exp_min', 'sim_tokens_num', 'words_freq_sim_max_1_den', 'words_freq_sim_max_1_diff', 'words_freq_sim_max_1_num', 'words_freq_sim_max_1_rel1', 'words_freq_sim_max_1_rel2', 'words_freq_sim_max_2_den', 'words_freq_sim_max_2_diff', 'words_freq_sim_max_2_num', 'words_freq_sim_max_2_rel1', 'words_freq_sim_max_2_rel2', 'words_freq_sim_max_3_den', 'words_freq_sim_max_3_diff', 'words_freq_sim_max_3_num', 'words_freq_sim_max_3_rel1', 'words_freq_sim_max_3_rel2', 'words_freq_sim_max_4_den', 'words_freq_sim_max_4_diff', 'words_freq_sim_max_4_num', 'words_freq_sim_max_4_rel1', 'words_freq_sim_max_4_rel2', 'words_freq_sim_max_5_den', 'words_freq_sim_max_5_diff', 'words_freq_sim_max_5_num', 'words_freq_sim_max_5_rel1', 'words_freq_sim_max_5_rel2', 'words_freq_sim_max_6_den', 'words_freq_sim_max_6_diff', 'words_freq_sim_max_6_num', 'words_freq_sim_max_6_rel1', 'words_freq_sim_max_6_rel2', 'words_freq_sim_min_1_den', 'words_freq_sim_min_1_diff', 'words_freq_sim_min_1_num', 'words_freq_sim_min_1_rel1', 'words_freq_sim_min_1_rel2', 'words_freq_sim_min_2_den', 'words_freq_sim_min_2_diff', 'words_freq_sim_min_2_num', 'words_freq_sim_min_2_rel1', 'words_freq_sim_min_2_rel2', 'words_freq_sim_min_3_den', 'words_freq_sim_min_3_diff', 'words_freq_sim_min_3_num', 'words_freq_sim_min_3_rel1', 'words_freq_sim_min_3_rel2', 'words_freq_sim_min_4_den', 'words_freq_sim_min_4_diff', 'words_freq_sim_min_4_num', 'words_freq_sim_min_4_rel1', 'words_freq_sim_min_4_rel2', 'words_freq_sim_min_5_den', 'words_freq_sim_min_5_diff', 'words_freq_sim_min_5_num', 'words_freq_sim_min_5_rel1', 'words_freq_sim_min_5_rel2', 'words_freq_sim_min_6_den', 'words_freq_sim_min_6_diff', 'words_freq_sim_min_6_num', 'words_freq_sim_min_6_rel1', 'words_freq_sim_min_6_rel2', 'words_freq_sim_sum_1_den', 'words_freq_sim_sum_1_diff', 'words_freq_sim_sum_1_num', 'words_freq_sim_sum_1_rel1', 'words_freq_sim_sum_1_rel2', 'words_freq_sim_sum_2_den', 'words_freq_sim_sum_2_diff', 'words_freq_sim_sum_2_num', 'words_freq_sim_sum_2_rel1', 'words_freq_sim_sum_2_rel2', 'words_freq_sim_sum_3_den', 'words_freq_sim_sum_3_diff', 'words_freq_sim_sum_3_num', 'words_freq_sim_sum_3_rel1', 'words_freq_sim_sum_3_rel2', 'words_freq_sim_sum_4_den', 'words_freq_sim_sum_4_diff', 'words_freq_sim_sum_4_num', 'words_freq_sim_sum_4_rel1', 'words_freq_sim_sum_4_rel2', 'words_freq_sim_sum_5_den', 'words_freq_sim_sum_5_diff', 'words_freq_sim_sum_5_num', 'words_freq_sim_sum_5_rel1', 'words_freq_sim_sum_5_rel2', 'words_freq_sim_sum_6_den', 'words_freq_sim_sum_6_diff', 'words_freq_sim_sum_6_num', 'words_freq_sim_sum_6_rel1', 'words_freq_sim_sum_6_rel2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_duplicate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_one_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164711</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_duplicate         0    1\n",
       "is_one_cluster             \n",
       "0               164711  593\n",
       "1                    5  631"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331879 165940\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_duplicate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164719</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_duplicate       0     1\n",
       "foo                       \n",
       "0             164719  1151\n",
       "1                 11    59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['cluster_size_max', 'cnt_neg_cluster', 'is_one_cluster', 'lcs', 'lcs1_norm', 'lcs2_norm', 'lcs3_norm', 'lcs_freq_norm', 'lcs_norm', 'lcs_norm_max', 'lcs_norm_sum', 'lcs_raw_norm', 'len_diff', 'len_diff_rel', 'len_max', 'len_min', 'len_t_diff', 'len_t_diff_rel', 'len_t_max', 'len_t_min', 'sim_tokens', 'sim_tokens_den', 'sim_tokens_exp_max', 'sim_tokens_exp_min', 'sim_tokens_num', 'words_freq_sim_max_1_den', 'words_freq_sim_max_1_diff', 'words_freq_sim_max_1_num', 'words_freq_sim_max_1_rel1', 'words_freq_sim_max_1_rel2', 'words_freq_sim_max_2_den', 'words_freq_sim_max_2_diff', 'words_freq_sim_max_2_num', 'words_freq_sim_max_2_rel1', 'words_freq_sim_max_2_rel2', 'words_freq_sim_max_3_den', 'words_freq_sim_max_3_diff', 'words_freq_sim_max_3_num', 'words_freq_sim_max_3_rel1', 'words_freq_sim_max_3_rel2', 'words_freq_sim_max_4_den', 'words_freq_sim_max_4_diff', 'words_freq_sim_max_4_num', 'words_freq_sim_max_4_rel1', 'words_freq_sim_max_4_rel2', 'words_freq_sim_max_5_den', 'words_freq_sim_max_5_diff', 'words_freq_sim_max_5_num', 'words_freq_sim_max_5_rel1', 'words_freq_sim_max_5_rel2', 'words_freq_sim_max_6_den', 'words_freq_sim_max_6_diff', 'words_freq_sim_max_6_num', 'words_freq_sim_max_6_rel1', 'words_freq_sim_max_6_rel2', 'words_freq_sim_min_1_den', 'words_freq_sim_min_1_diff', 'words_freq_sim_min_1_num', 'words_freq_sim_min_1_rel1', 'words_freq_sim_min_1_rel2', 'words_freq_sim_min_2_den', 'words_freq_sim_min_2_diff', 'words_freq_sim_min_2_num', 'words_freq_sim_min_2_rel1', 'words_freq_sim_min_2_rel2', 'words_freq_sim_min_3_den', 'words_freq_sim_min_3_diff', 'words_freq_sim_min_3_num', 'words_freq_sim_min_3_rel1', 'words_freq_sim_min_3_rel2', 'words_freq_sim_min_4_den', 'words_freq_sim_min_4_diff', 'words_freq_sim_min_4_num', 'words_freq_sim_min_4_rel1', 'words_freq_sim_min_4_rel2', 'words_freq_sim_min_5_den', 'words_freq_sim_min_5_diff', 'words_freq_sim_min_5_num', 'words_freq_sim_min_5_rel1', 'words_freq_sim_min_5_rel2', 'words_freq_sim_min_6_den', 'words_freq_sim_min_6_diff', 'words_freq_sim_min_6_num', 'words_freq_sim_min_6_rel1', 'words_freq_sim_min_6_rel2', 'words_freq_sim_sum_1_den', 'words_freq_sim_sum_1_diff', 'words_freq_sim_sum_1_num', 'words_freq_sim_sum_1_rel1', 'words_freq_sim_sum_1_rel2', 'words_freq_sim_sum_2_den', 'words_freq_sim_sum_2_diff', 'words_freq_sim_sum_2_num', 'words_freq_sim_sum_2_rel1', 'words_freq_sim_sum_2_rel2', 'words_freq_sim_sum_3_den', 'words_freq_sim_sum_3_diff', 'words_freq_sim_sum_3_num', 'words_freq_sim_sum_3_rel1', 'words_freq_sim_sum_3_rel2', 'words_freq_sim_sum_4_den', 'words_freq_sim_sum_4_diff', 'words_freq_sim_sum_4_num', 'words_freq_sim_sum_4_rel1', 'words_freq_sim_sum_4_rel2', 'words_freq_sim_sum_5_den', 'words_freq_sim_sum_5_diff', 'words_freq_sim_sum_5_num', 'words_freq_sim_sum_5_rel1', 'words_freq_sim_sum_5_rel2', 'words_freq_sim_sum_6_den', 'words_freq_sim_sum_6_diff', 'words_freq_sim_sum_6_num', 'words_freq_sim_sum_6_rel1', 'words_freq_sim_sum_6_rel2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_duplicate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_one_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164719</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_duplicate         0     1\n",
       "is_one_cluster              \n",
       "0               164719  1151\n",
       "1                   11    59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331880 165939\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_duplicate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164712</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_duplicate       0    1\n",
       "foo                      \n",
       "0             164712  943\n",
       "1                  3  281"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['cluster_size_max', 'cnt_neg_cluster', 'is_one_cluster', 'lcs', 'lcs1_norm', 'lcs2_norm', 'lcs3_norm', 'lcs_freq_norm', 'lcs_norm', 'lcs_norm_max', 'lcs_norm_sum', 'lcs_raw_norm', 'len_diff', 'len_diff_rel', 'len_max', 'len_min', 'len_t_diff', 'len_t_diff_rel', 'len_t_max', 'len_t_min', 'sim_tokens', 'sim_tokens_den', 'sim_tokens_exp_max', 'sim_tokens_exp_min', 'sim_tokens_num', 'words_freq_sim_max_1_den', 'words_freq_sim_max_1_diff', 'words_freq_sim_max_1_num', 'words_freq_sim_max_1_rel1', 'words_freq_sim_max_1_rel2', 'words_freq_sim_max_2_den', 'words_freq_sim_max_2_diff', 'words_freq_sim_max_2_num', 'words_freq_sim_max_2_rel1', 'words_freq_sim_max_2_rel2', 'words_freq_sim_max_3_den', 'words_freq_sim_max_3_diff', 'words_freq_sim_max_3_num', 'words_freq_sim_max_3_rel1', 'words_freq_sim_max_3_rel2', 'words_freq_sim_max_4_den', 'words_freq_sim_max_4_diff', 'words_freq_sim_max_4_num', 'words_freq_sim_max_4_rel1', 'words_freq_sim_max_4_rel2', 'words_freq_sim_max_5_den', 'words_freq_sim_max_5_diff', 'words_freq_sim_max_5_num', 'words_freq_sim_max_5_rel1', 'words_freq_sim_max_5_rel2', 'words_freq_sim_max_6_den', 'words_freq_sim_max_6_diff', 'words_freq_sim_max_6_num', 'words_freq_sim_max_6_rel1', 'words_freq_sim_max_6_rel2', 'words_freq_sim_min_1_den', 'words_freq_sim_min_1_diff', 'words_freq_sim_min_1_num', 'words_freq_sim_min_1_rel1', 'words_freq_sim_min_1_rel2', 'words_freq_sim_min_2_den', 'words_freq_sim_min_2_diff', 'words_freq_sim_min_2_num', 'words_freq_sim_min_2_rel1', 'words_freq_sim_min_2_rel2', 'words_freq_sim_min_3_den', 'words_freq_sim_min_3_diff', 'words_freq_sim_min_3_num', 'words_freq_sim_min_3_rel1', 'words_freq_sim_min_3_rel2', 'words_freq_sim_min_4_den', 'words_freq_sim_min_4_diff', 'words_freq_sim_min_4_num', 'words_freq_sim_min_4_rel1', 'words_freq_sim_min_4_rel2', 'words_freq_sim_min_5_den', 'words_freq_sim_min_5_diff', 'words_freq_sim_min_5_num', 'words_freq_sim_min_5_rel1', 'words_freq_sim_min_5_rel2', 'words_freq_sim_min_6_den', 'words_freq_sim_min_6_diff', 'words_freq_sim_min_6_num', 'words_freq_sim_min_6_rel1', 'words_freq_sim_min_6_rel2', 'words_freq_sim_sum_1_den', 'words_freq_sim_sum_1_diff', 'words_freq_sim_sum_1_num', 'words_freq_sim_sum_1_rel1', 'words_freq_sim_sum_1_rel2', 'words_freq_sim_sum_2_den', 'words_freq_sim_sum_2_diff', 'words_freq_sim_sum_2_num', 'words_freq_sim_sum_2_rel1', 'words_freq_sim_sum_2_rel2', 'words_freq_sim_sum_3_den', 'words_freq_sim_sum_3_diff', 'words_freq_sim_sum_3_num', 'words_freq_sim_sum_3_rel1', 'words_freq_sim_sum_3_rel2', 'words_freq_sim_sum_4_den', 'words_freq_sim_sum_4_diff', 'words_freq_sim_sum_4_num', 'words_freq_sim_sum_4_rel1', 'words_freq_sim_sum_4_rel2', 'words_freq_sim_sum_5_den', 'words_freq_sim_sum_5_diff', 'words_freq_sim_sum_5_num', 'words_freq_sim_sum_5_rel1', 'words_freq_sim_sum_5_rel2', 'words_freq_sim_sum_6_den', 'words_freq_sim_sum_6_diff', 'words_freq_sim_sum_6_num', 'words_freq_sim_sum_6_rel1', 'words_freq_sim_sum_6_rel2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_duplicate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_one_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164712</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_duplicate         0    1\n",
       "is_one_cluster             \n",
       "0               164712  943\n",
       "1                    3  281"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "tr_clusters = []\n",
    "tr_k2ind = []\n",
    "tr_freq = []\n",
    "tr_cl2cl_neg_list = []\n",
    "\n",
    "df_gr = []\n",
    "\n",
    "features = None\n",
    "#np.random.seed(777)\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "for train_index, test_index in group_kfold.split(train, groups=train['cv']):\n",
    "    print(len(train_index), len(test_index))\n",
    "    tr = train.iloc[train_index, :].reset_index(drop=True)\n",
    "    te = train.iloc[test_index, :].reset_index(drop=True)\n",
    "\n",
    "    clusters, k2ind, freq, cl2cl_neg = prepare(tr)\n",
    "    #clusters_simple, k2ind_simple, freq_simple, _ = prepare(tr, use_simple=True)\n",
    "\n",
    "    te['foo'] = [calc_is_one_cluster(e1, e2, clusters, k2ind) for e1, e2 in zip(te['name_1_tokens'], te['name_2_tokens'])]\n",
    "    \n",
    "    display(pd.crosstab(te['foo'], te['is_duplicate']))\n",
    "    \n",
    "    df_te, features = calc_features(te, [clusters], [k2ind], [freq], [cl2cl_neg])\n",
    "                                    #[clusters_simple], [k2ind_simple])\n",
    "    df_gr.append(df_te)\n",
    "    \n",
    "    display(pd.crosstab(df_te['is_one_cluster'], df_te['is_duplicate']))\n",
    "    \n",
    "    tr_clusters.append(clusters)\n",
    "    tr_k2ind.append(k2ind)\n",
    "    tr_freq.append(freq)\n",
    "    tr_cl2cl_neg_list.append(cl2cl_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9467730\ttest: 0.9311903\tbest: 0.9311903 (0)\ttotal: 223ms\tremaining: 22.1s\n",
      "10:\tlearn: 0.9610123\ttest: 0.9343348\tbest: 0.9343348 (10)\ttotal: 835ms\tremaining: 6.75s\n",
      "20:\tlearn: 0.9644749\ttest: 0.9361903\tbest: 0.9361903 (20)\ttotal: 1.5s\tremaining: 5.66s\n",
      "30:\tlearn: 0.9675724\ttest: 0.9325961\tbest: 0.9361903 (20)\ttotal: 2.15s\tremaining: 4.8s\n",
      "40:\tlearn: 0.9697006\ttest: 0.9359099\tbest: 0.9361903 (20)\ttotal: 2.76s\tremaining: 3.97s\n",
      "50:\tlearn: 0.9715173\ttest: 0.9379157\tbest: 0.9379157 (50)\ttotal: 3.39s\tremaining: 3.26s\n",
      "60:\tlearn: 0.9739005\ttest: 0.9390188\tbest: 0.9390188 (60)\ttotal: 4.01s\tremaining: 2.56s\n",
      "70:\tlearn: 0.9756465\ttest: 0.9391505\tbest: 0.9391505 (70)\ttotal: 4.61s\tremaining: 1.88s\n",
      "80:\tlearn: 0.9765670\ttest: 0.9408725\tbest: 0.9408725 (80)\ttotal: 5.19s\tremaining: 1.22s\n",
      "90:\tlearn: 0.9774560\ttest: 0.9422835\tbest: 0.9422835 (90)\ttotal: 5.81s\tremaining: 574ms\n",
      "99:\tlearn: 0.9788131\ttest: 0.9413860\tbest: 0.9422835 (90)\ttotal: 6.35s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9422834857\n",
      "bestIteration = 90\n",
      "\n",
      "('lcs', 8.096584232542968)\n",
      "('lcs1_norm', 7.289862064016149)\n",
      "('words_freq_sim_min_1_den', 6.04897079590556)\n",
      "('words_freq_sim_min_2_diff', 5.545253022847884)\n",
      "('words_freq_sim_min_3_num', 5.050909085506327)\n",
      "('words_freq_sim_min_3_den', 4.12124698922758)\n",
      "('words_freq_sim_sum_3_den', 3.3014103263946626)\n",
      "('words_freq_sim_min_3_rel1', 2.9954401362978476)\n",
      "('lcs2_norm', 2.6948887577282847)\n",
      "('words_freq_sim_sum_3_num', 2.386059682262393)\n",
      "(0.7570543738589224, 0.8424092409240925)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    164716\n",
      "           1       0.85      0.83      0.84      1224\n",
      "\n",
      "    accuracy                           1.00    165940\n",
      "   macro avg       0.92      0.92      0.92    165940\n",
      "weighted avg       1.00      1.00      1.00    165940\n",
      "\n",
      "203\n",
      "0:\tlearn: 0.9326503\ttest: 0.9010946\tbest: 0.9010946 (0)\ttotal: 62.4ms\tremaining: 6.18s\n",
      "10:\tlearn: 0.9547318\ttest: 0.9479579\tbest: 0.9479579 (10)\ttotal: 670ms\tremaining: 5.42s\n",
      "20:\tlearn: 0.9593440\ttest: 0.9489068\tbest: 0.9489068 (20)\ttotal: 1.27s\tremaining: 4.77s\n",
      "30:\tlearn: 0.9610408\ttest: 0.9511926\tbest: 0.9511926 (30)\ttotal: 1.84s\tremaining: 4.1s\n",
      "40:\tlearn: 0.9625404\ttest: 0.9526605\tbest: 0.9526605 (40)\ttotal: 2.44s\tremaining: 3.51s\n",
      "50:\tlearn: 0.9668105\ttest: 0.9556412\tbest: 0.9556412 (50)\ttotal: 3.03s\tremaining: 2.91s\n",
      "60:\tlearn: 0.9683439\ttest: 0.9570576\tbest: 0.9570576 (60)\ttotal: 3.63s\tremaining: 2.32s\n",
      "70:\tlearn: 0.9699098\ttest: 0.9592870\tbest: 0.9592870 (70)\ttotal: 4.23s\tremaining: 1.73s\n",
      "80:\tlearn: 0.9711786\ttest: 0.9605037\tbest: 0.9605037 (80)\ttotal: 4.82s\tremaining: 1.13s\n",
      "90:\tlearn: 0.9723474\ttest: 0.9608502\tbest: 0.9608502 (90)\ttotal: 5.45s\tremaining: 539ms\n",
      "99:\tlearn: 0.9740180\ttest: 0.9613357\tbest: 0.9613357 (99)\ttotal: 5.99s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9613357414\n",
      "bestIteration = 99\n",
      "\n",
      "('cluster_size_max', 11.756014106857611)\n",
      "('words_freq_sim_min_3_num', 7.705567020287999)\n",
      "('words_freq_sim_min_6_num', 6.383197163995371)\n",
      "('sim_tokens_exp_max', 6.285403268527413)\n",
      "('lcs1_norm', 5.561443254877411)\n",
      "('words_freq_sim_max_3_diff', 5.326240787354491)\n",
      "('words_freq_sim_min_1_den', 5.211498341457295)\n",
      "('words_freq_sim_min_5_diff', 4.856353510767001)\n",
      "('lcs', 3.6793477703764093)\n",
      "('lcs2_norm', 3.019964776342171)\n",
      "(0.8320943062361117, 0.8863070539419086)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    164730\n",
      "           1       0.89      0.88      0.89      1210\n",
      "\n",
      "    accuracy                           1.00    165940\n",
      "   macro avg       0.94      0.94      0.94    165940\n",
      "weighted avg       1.00      1.00      1.00    165940\n",
      "\n",
      "140\n",
      "0:\tlearn: 0.9381183\ttest: 0.9050173\tbest: 0.9050173 (0)\ttotal: 56ms\tremaining: 5.54s\n",
      "10:\tlearn: 0.9603309\ttest: 0.9405021\tbest: 0.9405021 (10)\ttotal: 625ms\tremaining: 5.06s\n",
      "20:\tlearn: 0.9629936\ttest: 0.9362318\tbest: 0.9405021 (10)\ttotal: 1.2s\tremaining: 4.51s\n",
      "30:\tlearn: 0.9614688\ttest: 0.9342050\tbest: 0.9405021 (10)\ttotal: 1.77s\tremaining: 3.95s\n",
      "40:\tlearn: 0.9637865\ttest: 0.9335136\tbest: 0.9405021 (10)\ttotal: 2.38s\tremaining: 3.42s\n",
      "50:\tlearn: 0.9693260\ttest: 0.9370841\tbest: 0.9405021 (10)\ttotal: 2.96s\tremaining: 2.84s\n",
      "60:\tlearn: 0.9710433\ttest: 0.9367019\tbest: 0.9405021 (10)\ttotal: 3.57s\tremaining: 2.28s\n",
      "70:\tlearn: 0.9726477\ttest: 0.9358680\tbest: 0.9405021 (10)\ttotal: 4.18s\tremaining: 1.71s\n",
      "80:\tlearn: 0.9743704\ttest: 0.9346485\tbest: 0.9405021 (10)\ttotal: 4.85s\tremaining: 1.14s\n",
      "90:\tlearn: 0.9751615\ttest: 0.9340933\tbest: 0.9405021 (10)\ttotal: 5.49s\tremaining: 543ms\n",
      "99:\tlearn: 0.9760766\ttest: 0.9330397\tbest: 0.9405021 (10)\ttotal: 6.08s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9405020606\n",
      "bestIteration = 10\n",
      "\n",
      "('words_freq_sim_min_3_num', 8.01652084724309)\n",
      "('words_freq_sim_min_1_den', 7.837442015673829)\n",
      "('lcs1_norm', 6.6045566365192965)\n",
      "('words_freq_sim_min_3_den', 4.555669205806222)\n",
      "('lcs_norm', 3.314020556896957)\n",
      "('words_freq_sim_max_6_den', 2.663180744345731)\n",
      "('lcs2_norm', 2.628155688716325)\n",
      "('words_freq_sim_sum_6_diff', 2.242541922762844)\n",
      "('words_freq_sim_min_3_rel2', 2.079462061758155)\n",
      "('words_freq_sim_max_3_den', 2.062657639322007)\n",
      "(0.7952559753974477, 0.846884028064383)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    164715\n",
      "           1       0.86      0.84      0.85      1224\n",
      "\n",
      "    accuracy                           1.00    165939\n",
      "   macro avg       0.93      0.92      0.92    165939\n",
      "weighted avg       1.00      1.00      1.00    165939\n",
      "\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "params = {\n",
    "    \"iterations\": 100,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"depth\": 6,\n",
    "    \"l2_leaf_reg\": 1.0,\n",
    "    \"rsm\": 0.9,\n",
    "    \"border_count\": 10,\n",
    "    \"max_ctr_complexity\": 2,\n",
    "    \"random_strength\": 1.0,\n",
    "    \"bagging_temperature\": 100.0,\n",
    "    \"grow_policy\": \"SymmetricTree\",\n",
    "    \"min_data_in_leaf\": 5,\n",
    "    \"langevin\": True,\n",
    "    \"diffusion_temperature\": 100000,\n",
    "    \"auto_class_weights\": 'SqrtBalanced',\n",
    "    \"random_seed\": 777\n",
    "}\n",
    "\n",
    "iter_scores = []\n",
    "\n",
    "for i in range(len(df_gr)):\n",
    "    df_tr = pd.concat([df_gr[j] for j in range(len(df_gr)) if i != j])\n",
    "    df_te = df_gr[i]\n",
    "\n",
    "    features_final = [e for e in features]\n",
    "\n",
    "    cb = CatBoostClassifier(**params, verbose=True, eval_metric='F1')\n",
    "    cb.fit(df_tr[features_final], df_tr['is_duplicate'],\n",
    "           eval_set=(df_te[features_final], df_te['is_duplicate']), metric_period=10, use_best_model=False)\n",
    "\n",
    "    for e in sorted(zip(features_final, cb.feature_importances_), key=lambda x: -x[1])[:10]:\n",
    "        print(e)\n",
    "        \n",
    "    pred = cb.predict_proba(df_te[features])[:, 1]\n",
    "    df_te['pred'] = pred\n",
    "\n",
    "    best_thr = df_te.sort_values('pred', ascending=False)['pred'].values[1200]\n",
    "    \n",
    "    best_score = f1_score(df_te['is_duplicate'], (pred > best_thr).astype(int))\n",
    "    print((best_thr, best_score))\n",
    "    print(classification_report(df_te['is_duplicate'], (pred > best_thr).astype(int)))\n",
    "    \n",
    "    iter_scores.append({\n",
    "        'best_thr': best_thr,\n",
    "        'best_score': best_score,\n",
    "        'df_te': df_te   \n",
    "    })\n",
    "    \n",
    "    print(len(df_te[(df_te['is_duplicate'] == 1) & (pred < best_thr)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_duplicate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_one_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494142</td>\n",
       "      <td>2687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_duplicate         0     1\n",
       "is_one_cluster              \n",
       "0               494142  2687"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tr_all = pd.concat(df_gr)\n",
    "df_tr_all = df_tr_all[(df_tr_all['is_one_cluster'] == 0)].reset_index(drop=True)\n",
    "display(pd.crosstab(df_tr_all['is_one_cluster'], df_tr_all['is_duplicate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_final = [e for e in features if e not in {\n",
    "    'is_one_cluster', 'cluster_size_max'\n",
    "}]\n",
    "len(features_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"iterations\": 100,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"depth\": 6,\n",
    "    \"l2_leaf_reg\": 1.0,\n",
    "    \"rsm\": 0.9,\n",
    "    \"border_count\": 10,\n",
    "    \"max_ctr_complexity\": 2,\n",
    "    \"random_strength\": 1.0,\n",
    "    \"bagging_temperature\": 100.0,\n",
    "    \"grow_policy\": \"SymmetricTree\",\n",
    "    \"min_data_in_leaf\": 5,\n",
    "    \"langevin\": True,\n",
    "    \"diffusion_temperature\": 100000,\n",
    "    \"auto_class_weights\": 'SqrtBalanced',\n",
    "    \"random_seed\": 777\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0:\tlearn: 0.9161167\ttotal: 91.5ms\tremaining: 9.05s\n",
      "10:\tlearn: 0.9392618\ttotal: 1.04s\tremaining: 8.42s\n",
      "20:\tlearn: 0.9426301\ttotal: 1.99s\tremaining: 7.5s\n",
      "30:\tlearn: 0.9475476\ttotal: 3.04s\tremaining: 6.77s\n",
      "40:\tlearn: 0.9499150\ttotal: 3.94s\tremaining: 5.67s\n",
      "50:\tlearn: 0.9550087\ttotal: 4.9s\tremaining: 4.71s\n",
      "60:\tlearn: 0.9593658\ttotal: 5.87s\tremaining: 3.75s\n",
      "70:\tlearn: 0.9622686\ttotal: 6.8s\tremaining: 2.78s\n",
      "80:\tlearn: 0.9639298\ttotal: 7.92s\tremaining: 1.86s\n",
      "90:\tlearn: 0.9651679\ttotal: 8.94s\tremaining: 884ms\n",
      "99:\tlearn: 0.9656934\ttotal: 9.8s\tremaining: 0us\n",
      "1\n",
      "0:\tlearn: 0.9029725\ttotal: 81.9ms\tremaining: 8.11s\n",
      "10:\tlearn: 0.9446329\ttotal: 967ms\tremaining: 7.83s\n",
      "20:\tlearn: 0.9475559\ttotal: 1.84s\tremaining: 6.94s\n",
      "30:\tlearn: 0.9495980\ttotal: 2.75s\tremaining: 6.11s\n",
      "40:\tlearn: 0.9518665\ttotal: 3.62s\tremaining: 5.2s\n",
      "50:\tlearn: 0.9589325\ttotal: 4.51s\tremaining: 4.34s\n",
      "60:\tlearn: 0.9605202\ttotal: 5.47s\tremaining: 3.5s\n",
      "70:\tlearn: 0.9628023\ttotal: 6.39s\tremaining: 2.61s\n",
      "80:\tlearn: 0.9644021\ttotal: 7.28s\tremaining: 1.71s\n",
      "90:\tlearn: 0.9657535\ttotal: 8.17s\tremaining: 808ms\n",
      "99:\tlearn: 0.9670636\ttotal: 8.99s\tremaining: 0us\n",
      "2\n",
      "0:\tlearn: 0.9199151\ttotal: 89ms\tremaining: 8.81s\n",
      "10:\tlearn: 0.9460425\ttotal: 936ms\tremaining: 7.57s\n",
      "20:\tlearn: 0.9496771\ttotal: 1.77s\tremaining: 6.68s\n",
      "30:\tlearn: 0.9589341\ttotal: 2.66s\tremaining: 5.91s\n",
      "40:\tlearn: 0.9593688\ttotal: 3.54s\tremaining: 5.09s\n",
      "50:\tlearn: 0.9615815\ttotal: 4.43s\tremaining: 4.26s\n",
      "60:\tlearn: 0.9629674\ttotal: 5.32s\tremaining: 3.4s\n",
      "70:\tlearn: 0.9635021\ttotal: 6.25s\tremaining: 2.55s\n",
      "80:\tlearn: 0.9643979\ttotal: 7.24s\tremaining: 1.7s\n",
      "90:\tlearn: 0.9646825\ttotal: 8.19s\tremaining: 810ms\n",
      "99:\tlearn: 0.9657940\ttotal: 9s\tremaining: 0us\n",
      "3\n",
      "0:\tlearn: 0.9134972\ttotal: 105ms\tremaining: 10.4s\n",
      "10:\tlearn: 0.9396517\ttotal: 1.03s\tremaining: 8.38s\n",
      "20:\tlearn: 0.9465069\ttotal: 1.9s\tremaining: 7.13s\n",
      "30:\tlearn: 0.9497567\ttotal: 2.82s\tremaining: 6.28s\n",
      "40:\tlearn: 0.9546798\ttotal: 3.7s\tremaining: 5.32s\n",
      "50:\tlearn: 0.9584471\ttotal: 4.61s\tremaining: 4.43s\n",
      "60:\tlearn: 0.9603343\ttotal: 5.59s\tremaining: 3.57s\n",
      "70:\tlearn: 0.9620711\ttotal: 6.52s\tremaining: 2.66s\n",
      "80:\tlearn: 0.9641878\ttotal: 7.5s\tremaining: 1.76s\n",
      "90:\tlearn: 0.9652804\ttotal: 8.43s\tremaining: 834ms\n",
      "99:\tlearn: 0.9659530\ttotal: 9.25s\tremaining: 0us\n",
      "4\n",
      "0:\tlearn: 0.9150068\ttotal: 88.7ms\tremaining: 8.79s\n",
      "10:\tlearn: 0.9448945\ttotal: 951ms\tremaining: 7.69s\n",
      "20:\tlearn: 0.9475956\ttotal: 1.85s\tremaining: 6.97s\n",
      "30:\tlearn: 0.9476750\ttotal: 2.71s\tremaining: 6.04s\n",
      "40:\tlearn: 0.9490420\ttotal: 3.58s\tremaining: 5.15s\n",
      "50:\tlearn: 0.9540140\ttotal: 4.42s\tremaining: 4.25s\n",
      "60:\tlearn: 0.9570254\ttotal: 5.33s\tremaining: 3.4s\n",
      "70:\tlearn: 0.9583215\ttotal: 6.32s\tremaining: 2.58s\n",
      "80:\tlearn: 0.9612009\ttotal: 7.2s\tremaining: 1.69s\n",
      "90:\tlearn: 0.9624113\ttotal: 8.09s\tremaining: 800ms\n",
      "99:\tlearn: 0.9645216\ttotal: 8.87s\tremaining: 0us\n",
      "5\n",
      "0:\tlearn: 0.9216574\ttotal: 79.8ms\tremaining: 7.9s\n",
      "10:\tlearn: 0.9424528\ttotal: 932ms\tremaining: 7.54s\n",
      "20:\tlearn: 0.9445428\ttotal: 1.8s\tremaining: 6.79s\n",
      "30:\tlearn: 0.9499946\ttotal: 2.85s\tremaining: 6.35s\n",
      "40:\tlearn: 0.9505625\ttotal: 3.94s\tremaining: 5.67s\n",
      "50:\tlearn: 0.9563661\ttotal: 4.99s\tremaining: 4.8s\n",
      "60:\tlearn: 0.9592845\ttotal: 5.97s\tremaining: 3.81s\n",
      "70:\tlearn: 0.9622122\ttotal: 6.88s\tremaining: 2.81s\n",
      "80:\tlearn: 0.9639012\ttotal: 7.96s\tremaining: 1.87s\n",
      "90:\tlearn: 0.9653008\ttotal: 8.93s\tremaining: 883ms\n",
      "99:\tlearn: 0.9663988\ttotal: 9.84s\tremaining: 0us\n",
      "6\n",
      "0:\tlearn: 0.9159880\ttotal: 93.2ms\tremaining: 9.22s\n",
      "10:\tlearn: 0.9407122\ttotal: 985ms\tremaining: 7.97s\n",
      "20:\tlearn: 0.9462728\ttotal: 1.84s\tremaining: 6.93s\n",
      "30:\tlearn: 0.9522521\ttotal: 2.76s\tremaining: 6.14s\n",
      "40:\tlearn: 0.9545249\ttotal: 3.68s\tremaining: 5.3s\n",
      "50:\tlearn: 0.9585496\ttotal: 4.66s\tremaining: 4.48s\n",
      "60:\tlearn: 0.9612539\ttotal: 5.6s\tremaining: 3.58s\n",
      "70:\tlearn: 0.9616703\ttotal: 6.49s\tremaining: 2.65s\n",
      "80:\tlearn: 0.9642316\ttotal: 7.4s\tremaining: 1.74s\n",
      "90:\tlearn: 0.9658791\ttotal: 8.3s\tremaining: 821ms\n",
      "99:\tlearn: 0.9663787\ttotal: 9.1s\tremaining: 0us\n",
      "7\n",
      "0:\tlearn: 0.9271319\ttotal: 94ms\tremaining: 9.31s\n",
      "10:\tlearn: 0.9403475\ttotal: 939ms\tremaining: 7.59s\n",
      "20:\tlearn: 0.9465176\ttotal: 1.78s\tremaining: 6.7s\n",
      "30:\tlearn: 0.9483672\ttotal: 2.65s\tremaining: 5.89s\n",
      "40:\tlearn: 0.9521961\ttotal: 3.53s\tremaining: 5.08s\n",
      "50:\tlearn: 0.9576887\ttotal: 4.4s\tremaining: 4.22s\n",
      "60:\tlearn: 0.9587978\ttotal: 5.29s\tremaining: 3.38s\n",
      "70:\tlearn: 0.9607189\ttotal: 6.15s\tremaining: 2.51s\n",
      "80:\tlearn: 0.9622786\ttotal: 7.06s\tremaining: 1.66s\n",
      "90:\tlearn: 0.9620048\ttotal: 7.91s\tremaining: 783ms\n",
      "99:\tlearn: 0.9638747\ttotal: 8.72s\tremaining: 0us\n",
      "8\n",
      "0:\tlearn: 0.9268737\ttotal: 84.7ms\tremaining: 8.38s\n",
      "10:\tlearn: 0.9375198\ttotal: 910ms\tremaining: 7.36s\n",
      "20:\tlearn: 0.9415592\ttotal: 1.76s\tremaining: 6.61s\n",
      "30:\tlearn: 0.9443570\ttotal: 2.6s\tremaining: 5.79s\n",
      "40:\tlearn: 0.9518647\ttotal: 3.52s\tremaining: 5.07s\n",
      "50:\tlearn: 0.9567444\ttotal: 4.45s\tremaining: 4.28s\n",
      "60:\tlearn: 0.9602157\ttotal: 5.39s\tremaining: 3.44s\n",
      "70:\tlearn: 0.9619936\ttotal: 6.36s\tremaining: 2.6s\n",
      "80:\tlearn: 0.9628980\ttotal: 7.29s\tremaining: 1.71s\n",
      "90:\tlearn: 0.9642029\ttotal: 8.2s\tremaining: 811ms\n",
      "99:\tlearn: 0.9660863\ttotal: 9.03s\tremaining: 0us\n",
      "9\n",
      "0:\tlearn: 0.9184202\ttotal: 98.5ms\tremaining: 9.76s\n",
      "10:\tlearn: 0.9408551\ttotal: 970ms\tremaining: 7.85s\n",
      "20:\tlearn: 0.9456487\ttotal: 1.85s\tremaining: 6.96s\n",
      "30:\tlearn: 0.9495715\ttotal: 2.7s\tremaining: 6s\n",
      "40:\tlearn: 0.9510918\ttotal: 3.55s\tremaining: 5.11s\n",
      "50:\tlearn: 0.9546991\ttotal: 4.42s\tremaining: 4.25s\n",
      "60:\tlearn: 0.9592656\ttotal: 5.34s\tremaining: 3.41s\n",
      "70:\tlearn: 0.9613999\ttotal: 6.26s\tremaining: 2.56s\n",
      "80:\tlearn: 0.9632748\ttotal: 7.14s\tremaining: 1.67s\n",
      "90:\tlearn: 0.9636677\ttotal: 8.02s\tremaining: 793ms\n",
      "99:\tlearn: 0.9648140\ttotal: 8.82s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "cbs = dict()\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    params['random_seed'] = i\n",
    "    cb = CatBoostClassifier(**params, verbose=True, eval_metric='F1')\n",
    "    cb.fit(df_tr_all[features_final], df_tr_all['is_duplicate'], metric_period=10)\n",
    "    cbs[i] = cb\n",
    "    \n",
    "#     check({i: cb})\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lcs1_norm', 10.836996578043953)\n",
      "('words_freq_sim_min_1_rel1', 9.786178011122107)\n",
      "('lcs', 9.437657717958901)\n",
      "('words_freq_sim_min_5_diff', 6.392731985589871)\n",
      "('words_freq_sim_min_1_den', 5.6364777650020645)\n",
      "('words_freq_sim_min_3_num', 4.383806156131153)\n",
      "('words_freq_sim_min_3_diff', 3.925656267737122)\n",
      "('words_freq_sim_min_3_den', 3.3945231886992246)\n",
      "('words_freq_sim_min_3_rel1', 3.190884986129143)\n",
      "('lcs2_norm', 2.358971847368973)\n",
      "('lcs_norm', 2.2557746055435817)\n",
      "('lcs_raw_norm', 1.8751969460497075)\n",
      "('words_freq_sim_sum_1_num', 1.5623252447948135)\n",
      "('words_freq_sim_max_3_den', 1.5516408599644305)\n",
      "('words_freq_sim_min_4_diff', 1.4906749689136913)\n",
      "('words_freq_sim_max_6_num', 1.399732928133153)\n",
      "('words_freq_sim_max_5_den', 1.2434633796984678)\n",
      "('words_freq_sim_max_4_den', 1.2172952660007752)\n",
      "('words_freq_sim_sum_6_num', 1.1829128716331652)\n",
      "('words_freq_sim_sum_1_diff', 1.0447210102283606)\n",
      "('words_freq_sim_sum_5_num', 1.029237263219043)\n",
      "('words_freq_sim_min_2_num', 0.9321574977434453)\n",
      "('words_freq_sim_min_3_rel2', 0.916577874343235)\n",
      "('words_freq_sim_max_1_den', 0.893068180305023)\n",
      "('words_freq_sim_max_6_den', 0.8855862919591491)\n",
      "('len_t_max', 0.8834593131672059)\n",
      "('words_freq_sim_min_2_diff', 0.8513253560031858)\n",
      "('words_freq_sim_min_5_den', 0.8473079607213717)\n",
      "('words_freq_sim_sum_5_den', 0.7922568066213675)\n",
      "('words_freq_sim_sum_4_den', 0.7635750958783166)\n",
      "('words_freq_sim_max_2_rel1', 0.7196764618601234)\n",
      "('words_freq_sim_max_6_diff', 0.6846610292316984)\n",
      "('words_freq_sim_sum_3_den', 0.6652929644514134)\n",
      "('words_freq_sim_max_6_rel1', 0.660374951807517)\n",
      "('words_freq_sim_sum_6_den', 0.6556371825885184)\n",
      "('words_freq_sim_min_2_den', 0.5928885987350271)\n",
      "('words_freq_sim_min_5_num', 0.5476467662979607)\n",
      "('words_freq_sim_min_6_diff', 0.5412135836971391)\n",
      "('words_freq_sim_max_2_num', 0.5241419478075148)\n",
      "('words_freq_sim_sum_2_den', 0.5205853409129932)\n",
      "('words_freq_sim_max_4_num', 0.519016417637057)\n",
      "('words_freq_sim_sum_3_num', 0.49641724724654096)\n",
      "('words_freq_sim_max_3_num', 0.49048461234818236)\n",
      "('words_freq_sim_sum_2_rel2', 0.4900017887039282)\n",
      "('words_freq_sim_max_3_rel1', 0.47706047142309205)\n",
      "('words_freq_sim_min_5_rel1', 0.46337712642618695)\n",
      "('len_t_min', 0.44604881131948726)\n",
      "('words_freq_sim_min_4_den', 0.4238735381860602)\n",
      "('words_freq_sim_sum_2_diff', 0.4023513082444573)\n",
      "('words_freq_sim_sum_4_num', 0.37674900673981804)\n",
      "('words_freq_sim_min_5_rel2', 0.3664655190441018)\n",
      "('words_freq_sim_sum_6_rel1', 0.361087310672752)\n",
      "('words_freq_sim_max_5_num', 0.350094077962221)\n",
      "('words_freq_sim_min_6_num', 0.3235173272523641)\n",
      "('words_freq_sim_max_3_diff', 0.30899594341822983)\n",
      "('sim_tokens_exp_max', 0.2918534422088341)\n",
      "('words_freq_sim_sum_4_diff', 0.261885818554672)\n",
      "('words_freq_sim_sum_2_rel1', 0.26058399450418734)\n",
      "('words_freq_sim_sum_6_diff', 0.25866262320730943)\n",
      "('words_freq_sim_sum_1_rel2', 0.24905580876463543)\n",
      "('lcs3_norm', 0.2435081506725566)\n",
      "('words_freq_sim_min_1_diff', 0.22833134434741062)\n",
      "('words_freq_sim_max_1_num', 0.2196750493146965)\n",
      "('words_freq_sim_max_6_rel2', 0.20641838455300354)\n",
      "('words_freq_sim_min_6_den', 0.1946704445547479)\n",
      "('words_freq_sim_sum_1_den', 0.1887387331416977)\n",
      "('cnt_neg_cluster', 0.18748004852427336)\n",
      "('words_freq_sim_min_4_num', 0.16892481040004007)\n",
      "('words_freq_sim_sum_5_rel2', 0.1650036605941939)\n",
      "('sim_tokens_exp_min', 0.1634725570945076)\n",
      "('len_max', 0.1502815238304929)\n",
      "('words_freq_sim_max_1_rel2', 0.1106109088049289)\n",
      "('words_freq_sim_max_3_rel2', 0.10440725479260216)\n",
      "('lcs_norm_max', 0.09913628483584291)\n",
      "('words_freq_sim_max_2_rel2', 0.09625959103308605)\n",
      "('words_freq_sim_sum_4_rel1', 0.09352724104695392)\n",
      "('words_freq_sim_sum_5_diff', 0.08330648766141603)\n",
      "('lcs_freq_norm', 0.08194642169049539)\n",
      "('sim_tokens', 0.07348497061919933)\n",
      "('words_freq_sim_max_1_rel1', 0.07083377900037349)\n",
      "('words_freq_sim_max_4_rel2', 0.04325950514252085)\n",
      "('words_freq_sim_sum_3_diff', 0.03722345290384893)\n",
      "('words_freq_sim_max_5_rel2', 0.036383120483729964)\n",
      "('len_min', 0.03398565241953175)\n",
      "('words_freq_sim_sum_6_rel2', 0.0339208826779567)\n",
      "('words_freq_sim_min_1_num', 0.03349776035135201)\n",
      "('sim_tokens_den', 0.03230372356829294)\n",
      "('words_freq_sim_max_1_diff', 0.024986149990411773)\n",
      "('words_freq_sim_sum_3_rel1', 0.024320053016923398)\n",
      "('words_freq_sim_max_4_rel1', 0.02413945777442029)\n",
      "('words_freq_sim_sum_5_rel1', 0.021270989224821003)\n",
      "('words_freq_sim_min_4_rel1', 0.020634312618158346)\n",
      "('words_freq_sim_max_5_rel1', 0.012182047357476758)\n",
      "('lcs_norm_sum', 0.0)\n",
      "('len_diff', 0.0)\n",
      "('len_diff_rel', 0.0)\n",
      "('len_t_diff', 0.0)\n",
      "('len_t_diff_rel', 0.0)\n",
      "('sim_tokens_num', 0.0)\n",
      "('words_freq_sim_max_2_den', 0.0)\n",
      "('words_freq_sim_max_2_diff', 0.0)\n",
      "('words_freq_sim_max_4_diff', 0.0)\n",
      "('words_freq_sim_max_5_diff', 0.0)\n",
      "('words_freq_sim_min_1_rel2', 0.0)\n",
      "('words_freq_sim_min_2_rel1', 0.0)\n",
      "('words_freq_sim_min_2_rel2', 0.0)\n",
      "('words_freq_sim_min_4_rel2', 0.0)\n",
      "('words_freq_sim_min_6_rel1', 0.0)\n",
      "('words_freq_sim_min_6_rel2', 0.0)\n",
      "('words_freq_sim_sum_1_rel1', 0.0)\n",
      "('words_freq_sim_sum_2_num', 0.0)\n",
      "('words_freq_sim_sum_3_rel2', 0.0)\n",
      "('words_freq_sim_sum_4_rel2', 0.0)\n"
     ]
    }
   ],
   "source": [
    "for e in sorted(zip(features_final, cbs[0].feature_importances_), key=lambda x: -x[1]):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прогнозы на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_DIR.joinpath('test.csv'), index_col=\"pair_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for i, s in enumerate(set(test['name_1']) | set(test['name_2'])):\n",
    "    if s not in docs:\n",
    "        docs[s] = nlp_en(s)\n",
    "        k += 1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "new_tokens = set()\n",
    "for e in docs:\n",
    "    for t in simple_transform(e).split():\n",
    "        if t not in all_tokens:\n",
    "            new_tokens.add(t)\n",
    "print(len(new_tokens))\n",
    "for token in new_tokens:\n",
    "    docs_tokens[token] = nlp_en(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879 507\n"
     ]
    }
   ],
   "source": [
    "geo_add = set()\n",
    "kk = 0\n",
    "for token in docs_tokens:\n",
    "    doc = docs_tokens[token]\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'GPE':\n",
    "            geo_add.add(token)\n",
    "            if token not in cities_alt and token not in countries:\n",
    "                #print(token)\n",
    "                kk += 1\n",
    "\n",
    "print(len(geo_add), kk)\n",
    "geo_re = multi_str_replace([rf\"{entity}\" for entity in geo_add], debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name_1\n",
      "name_2\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for col in ['name_1', 'name_2']:\n",
    "    print(col)\n",
    "    test[f'{col}_tokens'] = [tokenize(e) for e in test[col]]\n",
    "    test[f'{col}_tokens_with_br'] = [tokenize(e, del_brackets=False) for e in test[col]]\n",
    "    test[f'{col}_tokens_simple'] = [simple_transform(e).split() for e in test[col]] \n",
    "#     test[f'{col}_tokens_org'] = [tokenize(e, del_brackets=True, only_org=True) for e in test[col]]\n",
    "    test[f'{col}_tokens_freq'] = [tokenize(e, del_brackets=True, only_org=False, use_freq=True) for e in test[col]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['cluster_size_max', 'cnt_neg_cluster', 'is_one_cluster', 'lcs', 'lcs1_norm', 'lcs2_norm', 'lcs3_norm', 'lcs_freq_norm', 'lcs_norm', 'lcs_norm_max', 'lcs_norm_sum', 'lcs_raw_norm', 'len_diff', 'len_diff_rel', 'len_max', 'len_min', 'len_t_diff', 'len_t_diff_rel', 'len_t_max', 'len_t_min', 'sim_tokens', 'sim_tokens_den', 'sim_tokens_exp_max', 'sim_tokens_exp_min', 'sim_tokens_num', 'words_freq_sim_max_1_den', 'words_freq_sim_max_1_diff', 'words_freq_sim_max_1_num', 'words_freq_sim_max_1_rel1', 'words_freq_sim_max_1_rel2', 'words_freq_sim_max_2_den', 'words_freq_sim_max_2_diff', 'words_freq_sim_max_2_num', 'words_freq_sim_max_2_rel1', 'words_freq_sim_max_2_rel2', 'words_freq_sim_max_3_den', 'words_freq_sim_max_3_diff', 'words_freq_sim_max_3_num', 'words_freq_sim_max_3_rel1', 'words_freq_sim_max_3_rel2', 'words_freq_sim_max_4_den', 'words_freq_sim_max_4_diff', 'words_freq_sim_max_4_num', 'words_freq_sim_max_4_rel1', 'words_freq_sim_max_4_rel2', 'words_freq_sim_max_5_den', 'words_freq_sim_max_5_diff', 'words_freq_sim_max_5_num', 'words_freq_sim_max_5_rel1', 'words_freq_sim_max_5_rel2', 'words_freq_sim_max_6_den', 'words_freq_sim_max_6_diff', 'words_freq_sim_max_6_num', 'words_freq_sim_max_6_rel1', 'words_freq_sim_max_6_rel2', 'words_freq_sim_min_1_den', 'words_freq_sim_min_1_diff', 'words_freq_sim_min_1_num', 'words_freq_sim_min_1_rel1', 'words_freq_sim_min_1_rel2', 'words_freq_sim_min_2_den', 'words_freq_sim_min_2_diff', 'words_freq_sim_min_2_num', 'words_freq_sim_min_2_rel1', 'words_freq_sim_min_2_rel2', 'words_freq_sim_min_3_den', 'words_freq_sim_min_3_diff', 'words_freq_sim_min_3_num', 'words_freq_sim_min_3_rel1', 'words_freq_sim_min_3_rel2', 'words_freq_sim_min_4_den', 'words_freq_sim_min_4_diff', 'words_freq_sim_min_4_num', 'words_freq_sim_min_4_rel1', 'words_freq_sim_min_4_rel2', 'words_freq_sim_min_5_den', 'words_freq_sim_min_5_diff', 'words_freq_sim_min_5_num', 'words_freq_sim_min_5_rel1', 'words_freq_sim_min_5_rel2', 'words_freq_sim_min_6_den', 'words_freq_sim_min_6_diff', 'words_freq_sim_min_6_num', 'words_freq_sim_min_6_rel1', 'words_freq_sim_min_6_rel2', 'words_freq_sim_sum_1_den', 'words_freq_sim_sum_1_diff', 'words_freq_sim_sum_1_num', 'words_freq_sim_sum_1_rel1', 'words_freq_sim_sum_1_rel2', 'words_freq_sim_sum_2_den', 'words_freq_sim_sum_2_diff', 'words_freq_sim_sum_2_num', 'words_freq_sim_sum_2_rel1', 'words_freq_sim_sum_2_rel2', 'words_freq_sim_sum_3_den', 'words_freq_sim_sum_3_diff', 'words_freq_sim_sum_3_num', 'words_freq_sim_sum_3_rel1', 'words_freq_sim_sum_3_rel2', 'words_freq_sim_sum_4_den', 'words_freq_sim_sum_4_diff', 'words_freq_sim_sum_4_num', 'words_freq_sim_sum_4_rel1', 'words_freq_sim_sum_4_rel2', 'words_freq_sim_sum_5_den', 'words_freq_sim_sum_5_diff', 'words_freq_sim_sum_5_num', 'words_freq_sim_sum_5_rel1', 'words_freq_sim_sum_5_rel2', 'words_freq_sim_sum_6_den', 'words_freq_sim_sum_6_diff', 'words_freq_sim_sum_6_num', 'words_freq_sim_sum_6_rel1', 'words_freq_sim_sum_6_rel2']\n"
     ]
    }
   ],
   "source": [
    "df_te_all_all, _ = calc_features(test, tr_clusters, tr_k2ind, tr_freq, tr_cl2cl_neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_te_all_all['pred'] = 0\n",
    "k = 0\n",
    "for i in cbs:\n",
    "    pred = cbs[i].predict_proba(df_te_all_all[features_final], ntree_end=80)[:, 1]\n",
    "    df_te_all_all[f'pred{i}'] = pred\n",
    "    df_te_all_all['pred'] += pred / len(cbs)\n",
    "    k += 1\n",
    "#     if i == 0:\n",
    "#         break\n",
    "\n",
    "#df_te_all_all['pred_final'] =  df_te_all_all['pred']\n",
    "df_te_all_all['pred_final'] =  df_te_all_all[[f'pred{i}' for i in range(k)]].min(axis=1)\n",
    "\n",
    "topn = 1600\n",
    "\n",
    "col = 'pred_final'\n",
    "df_te_all_all.loc[df_te_all_all['is_one_cluster'] == 1, col] = 1\n",
    "thr_topn = df_te_all_all.sort_values(col)[col].values[::-1][topn]\n",
    "df_te_all_all['is_duplicate'] = (df_te_all_all[col] > thr_topn).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_te_all_all[['is_duplicate']].to_csv('subm_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
